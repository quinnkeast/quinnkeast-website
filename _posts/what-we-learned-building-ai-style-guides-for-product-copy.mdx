---
title: "What we learned building AI style guides for product copy"
subtitle: "Teaching machines to follow rules they don’t want to follow"
date: "2025-08-27"
author:
  name: Quinn Keast
  picture: "/assets/quinn-portrait.jpg"
ogImage:
  url: "/assets/writing/emilipothese-R4WCbazrD1g-unsplash.jpg"
published: true
tags:
  - LLMs
  - AI
  - thinking
---

I’ve been working on designing Ditto’s AI content system for the last few months (among a huge pile of other things that made up <a href="https://www.dittowords.com/post/ditto-2-0-the-why-behind-the-launch" target="_blank">Ditto 2.0</a>).

I’m a huge fan of product style guides. I think the words we use in products have a wildly outsized ROI compared to almost everything else in design, because every word packs in a crazy amount of symbolic meaning. Years ago, I launched the <a href="https://uxlanguage.com" target="_blank">Product Language Framework</a>, a set of UX copywriting and style guidelines that can be used as-is for everyday reference, or teams can clone it and use it as the basis for creating and deploying their own style guide.

The moment ChatGPT 3.0 hit, I wondered if a LLM could be used to automatically lint your product copy against your style guide. I tried it out and it seemed promising, but annoying. Component libraries and design tokens are deeply integrated into the designer and developer tool stack, so they “just work” as part of the regular workflow. Anything to do with a LLM seemed like a bunch of extra steps that would be hard to pull off in a way that people would actually use in their workflows.

I joined <a href="https://dittowords.com" target="_blank">Ditto</a> last year because they were thinking very deeply about how to systemize product copy along with everything else we’ve systemized in the course of building products. I had an inkling that this was the kind of platform we’d need to get style guides integrated into the actual design and development workflows.

But we’ve been careful about how we approach AI at Ditto. The last thing we want to do is put <a href="https://blog.foxtrotluna.social/theyre-putting-blue-food-coloring-in-everything" target="_blank">blue food coloring</a> in it. Even setting aside that we were deep into the biggest product overhaul we’d ever done, we wanted to make sure we were intentional about providing real, meaningful value with any AI features.

The right moment came not long after Figma Config, where we soft launched Ditto 2.0 and talked with a lot of curious teams. We built and shipped Ditto’s AI content system in a matter of weeks. On the design side, we ran into some interesting challenges I thought I’d share.

First, a bit of conceptual underpinning. Ditto’s AI content system is a mix of:

- **Style guides**. These are made up of rules, with a name, description, and set of examples. A team can have multiple separate style guides, managed through the Ditto web app, and each style guide can be represented in JSON.
- **Magic edit**. Any time a text layer is selected in Figma or in the Ditto web app, we “lint” the text against the style guides using a LLM. If there’s anything about the text that breaks a rule in the style guide, we generate a suggested edit that’s in itself aligned to the style guide, and explain why in a way that ties back to the source rule.
- **Magic draft**. Any time a text layer is selected in Figma or in the Ditto web app, the user can trigger magic draft to help write what I’ve been calling “the draft and a half.” Ditto will take the context of the existing text, other text in the design, any additional metadata captured by Ditto’s platform, and generate a suggestion that combines the user’s input together with that context and the style guide.

_(Screenshots to come)_

With those concepts in place…

### LLMs really, truly, want to do shit in their own way

We knew going in that the non-deterministic nature of a LLM could be a challenge. Some of our earliest tests gave us confidence that it was feasible, but the more we experimented, the more trouble we ran into. It turns out that it’s very, very hard to align LLMs to the task of checking short product copy against style guides without injecting their own “opinions.”

This makes sense because most content that LLMs are trained on is not written against your style guide, but rather a huge corpus of text that follows other style guides or no style guide at all. Getting a LLM to follow your specific style guide is somewhat asking it to work against its own probabilities.

But we were really surprised by what things ended up being most challenging.

It’s total hell to make an LLM correctly use dumb or typographic quotes. This is a totally valid use case for automated style guides that’s almost impossible to pull off manually, but would be a perfect detail for an AI content system to handle. But boy—it just found so many ways to ignore it, work around it, and in one memorable case, change a rule to the literal opposite of what the actual rule stated just to avoid the correct use of quotes.

_(Screenshots to come)_

### False positives (a suggestion that isn’t useful or is wrong) are way worse in aggregate than false negatives (a missed suggestion)

This at first felt counterintuitive, but the more we tested, the more we discovered that it was more important to avoid triggering unhelpful suggestions than it was to avoid missing a broken rule.

The thing about product copy is there’s a lot of it. Every string in your product is a piece of copy. Every time you select a text layer in Figma or in the web app, we lint it. And if even 10% of those trigger a false positive, then in no time at all, users lose trust and develop “banner blindness” to it, and won’t notice at all when there’s a real, legitimate catch.

The risk of a missed suggestion is near nil.

### Style guide rules are hard

We researched dozens of style guides and found a huge range in the types of rules that they contained.

For example, my own Product Language Framework has:

- <a href="https://www.uxlanguage.com/style-and-mechanics#conversational-writing" target="_blank">General rules of thumb</a>, like “use conversational writing”
- <a href="https://www.uxlanguage.com/style-and-mechanics#punctuation" target="_blank">Style and mechanics</a>, like “use this region’s date format” or “use en dashes between ranges”
- <a href="https://www.uxlanguage.com/actionable-language#buttons" target="_blank">UI-element-specific guidelines</a>, like “button labels should follow the formula verb + noun”
- <a href="https://www.uxlanguage.com/word-list" target="_blank">Word lists</a>

Many of these rules contradict each other depending on context, and other rules have edge cases that that contradict themselves. Consider: one should use active voice rather than passive voice. Unless, of course, one aims to avoid assigning responsibility for an action.

Our rule system and way of handling examples needed to support all of these rules and examples intuitively, and integrate well with our use of LLMs—remembering that every rule would be part of the same context.

As a bonus problem, we needed to design a default style guide available to all workspaces, which would let everyone encounter the potential value of the AI content system—meaning it had to reliably catch occasional issues that most product content will have, but not *too many* issues.

### Style guides for AI content systems are not style guides for human consumption

Most style guides that act as standalone artifacts for reference are formatted and organized for human consumption. Someone goes about content design for them. My own Product Language Framework tries to be easy to consume with structure and categories and nested rules. The first version of our guidelines needed to be more minimalistic, because we needed folks to focus on creating good rules rather than designing content for human consumption.

### Prompt engineering is hilarious

“You sound like you’re giving instructions to a small child.”

### It’s hard to be both valuable <u>and</u> unintrusive

So, so many products have just crammed AI features in wherever they fit to ride the AI hype wave. Based on what I’ve seen of the sales process these days, this isn’t surprising.

But with Ditto we wanted to be very thoughtful around the fine line between “useful value surfaced proactively” and “annoying, untrustworthy crap that’s just another example of a product cramming AI into everything.”

This was both a UI design challenge and systems challenge. Ultimately it came down to testing. Testing for this kind of product space is inherently subjective and iterative. Somewhere along the lines a human has to observe and evaluate the suggestions and decide if they’re actually good suggestions, and then figure out what to do about that.

The feedback loop needed for “drafting and iterating on product copy” is dramatically different from “testing and iterating on the product that helps to draft and iterate on product copy.” We needed to be able to test both quality (good suggestions for real issues) and reliability (consistently good suggestions for real issues).

We ended up building a cool internal tool to test and iterate on our rules and the LLM integration at scale. We’d make a tweak, then run the linting against dozens of known strings, repeatedly, and then we could visually identify themes and decide what to do about it.

_(Screenshot to come)_

---

Magic draft is coming up soon, and it’s going to be a lot of fun to keep iterating on this system.

My colleague Reed Barnes wrote another article <a href="https://reedtaylorbarnes.com/blog/building-with-llms/" target="_blank">sharing some advice around building on top of an LLM from the engineering perspective</a>.
